{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\n",
      "  Downloading tensorboardX-2.2-py2.py3-none-any.whl (120 kB)\n",
      "\u001b[K     |████████████████████████████████| 120 kB 628 kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages (from tensorboardX) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/anaconda3/envs/tensorflow/lib/python3.6/site-packages (from tensorboardX) (3.14.0)\n",
      "Requirement already satisfied: six>=1.9 in /Users/valentinibars/.local/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load train set and test set and normalize the images in range [-1,1]\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(( 0.1307,), ( 0.3081,))])\n",
    "\n",
    "#50000 images training\n",
    "trainset = torchvision.datasets.MNIST(root='./mnist', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "#We load 4 samples per batch reduce the traininset to 12500\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "#print(len(trainset))\n",
    "#10000 images test\n",
    "testset = torchvision.datasets.MNIST(root='./mnist', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "#We load 4 samples per batchreduce the traininset to 2500\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "classes = list(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "        # Spatial transformer localization-network\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(10 * 3 * 3, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "    # Spatial transformer network forward function\n",
    "    def stn(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * 3 * 3)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # transform the input\n",
    "        x = self.stn(x)\n",
    "\n",
    "        # Perform the usual forward pass\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.001, accuracy: 0.000\n",
      "[1,   501] loss: 0.575, accuracy: 11.450\n",
      "[1,  1001] loss: 0.575, accuracy: 11.300\n",
      "[1,  1501] loss: 0.576, accuracy: 10.300\n",
      "[1,  2001] loss: 0.575, accuracy: 11.700\n",
      "[1,  2501] loss: 0.575, accuracy: 11.900\n",
      "[1,  3001] loss: 0.576, accuracy: 11.450\n",
      "[1,  3501] loss: 0.575, accuracy: 11.250\n",
      "[1,  4001] loss: 0.575, accuracy: 11.750\n",
      "[1,  4501] loss: 0.575, accuracy: 11.350\n",
      "[1,  5001] loss: 0.575, accuracy: 11.650\n",
      "[1,  5501] loss: 0.575, accuracy: 11.500\n",
      "[1,  6001] loss: 0.576, accuracy: 10.350\n",
      "[1,  6501] loss: 0.576, accuracy: 10.900\n",
      "[1,  7001] loss: 0.575, accuracy: 11.850\n",
      "[1,  7501] loss: 0.574, accuracy: 11.000\n",
      "[1,  8001] loss: 0.576, accuracy: 11.150\n",
      "[1,  8501] loss: 0.576, accuracy: 12.050\n",
      "[1,  9001] loss: 0.576, accuracy: 10.700\n",
      "[1,  9501] loss: 0.576, accuracy: 9.800\n",
      "[1, 10001] loss: 0.575, accuracy: 10.850\n",
      "[1, 10501] loss: 0.575, accuracy: 11.900\n",
      "[1, 11001] loss: 0.576, accuracy: 10.500\n",
      "[1, 11501] loss: 0.575, accuracy: 11.850\n",
      "[1, 12001] loss: 0.576, accuracy: 10.250\n",
      "[1, 12501] loss: 0.576, accuracy: 9.250\n",
      "[1, 13001] loss: 0.575, accuracy: 10.700\n",
      "[1, 13501] loss: 0.575, accuracy: 10.950\n",
      "[1, 14001] loss: 0.575, accuracy: 12.050\n",
      "[1, 14501] loss: 0.576, accuracy: 11.550\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "model.train()\n",
    "total = 0 \n",
    "correct = 0\n",
    "running_loss = 0\n",
    "for epoch in range(1, 1 + 1): \n",
    "    model.train()\n",
    "    writer = SummaryWriter(logdir='TransfNet_MNIST/training_epoch%d'%(epoch))\n",
    "    for i, datas in enumerate(trainloader):\n",
    "        inputs, labels = datas\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = F.nll_loss(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _,predicted=torch.max(output,1)\n",
    "        running_loss += loss.item()\n",
    "        total += labels.size(0)#count the number of labels with right shape\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if i % 500 == 0:\n",
    "            print('[%d, %5d] loss: %.3f, accuracy: %.3f' %\n",
    "                  (epoch , i + 1, running_loss / 2000, 100*float(correct)/float(total)))\n",
    "            writer.add_scalar('accuracy_train', 100*float(correct)/float(total), i)\n",
    "            writer.add_scalar('loss_train', running_loss / 500, i)\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "    writer.close()\n",
    "print ('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training saving\n",
    "PATH = './MNIST_TransformerNet_16epochs.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valentinibars/opt/anaconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  500] loss: 4.603, accuracy: 46.800\n",
      "[ 1000] loss: 4.605, accuracy: 43.200\n",
      "[ 1500] loss: 4.603, accuracy: 47.200\n",
      "[ 2000] loss: 4.605, accuracy: 43.800\n",
      "[ 2500] loss: 4.601, accuracy: 46.000\n",
      "Accuracy of the network on the 10000 test images: 45 %\n"
     ]
    }
   ],
   "source": [
    "PATH = './MNIST_TransformerNet_16epochs.pth'\n",
    "#Load previously net from choosen training\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "writer = SummaryWriter(logdir='TransfNet_MNIST/test')\n",
    "with torch.no_grad():#desactivate autograd engine ( used to perform validation )    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    correct_final = 0\n",
    "    total = 0\n",
    "    total_final = 0\n",
    "    \n",
    "    for i, datas in enumerate(testloader):\n",
    "        inputs, labels = datas\n",
    "        output = model(inputs)\n",
    "        loss = F.nll_loss(output, labels, size_average=False)\n",
    "        # sum up batch loss\n",
    "        test_loss += loss.item()\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        total += labels.size(0) #count the number of labels with right shape\n",
    "        total_final += labels.size(0) #count the number of labels with right shape\n",
    "        correct += (pred == labels).sum().item() #count the number of right labels\n",
    "        correct_final += (pred == labels).sum().item() #count the number of right labels\n",
    "        test_loss += loss.item()\n",
    "        #print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct,len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
    "        if i % 500 == 499:    # print every 500 mini-batches\n",
    "            print('[%5d] loss: %.3f, accuracy: %.3f' %( i + 1, (test_loss / 2000), (100*correct/total) ))\n",
    "        \n",
    "            writer.add_scalar('accuracy_test', 100*correct/total, i)\n",
    "            writer.add_scalar('loss_test', test_loss / 2000, i)\n",
    "        \n",
    "            test_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "writer.close()\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct_final / total_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "930e46df0d1b93ecf8fe51f04b6e478dce688000f1283825e61bc5e483dc45d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
