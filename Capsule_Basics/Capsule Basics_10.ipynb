{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Load train set and test set and normalize the images in range [-1,1]\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#50000 images training\n",
    "trainset = torchvision.datasets.CIFAR10(root='./dataCaps', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "#We load 4 samples per batch reduce the traininset to 12500\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "#print(len(trainset))\n",
    "#10000 images test\n",
    "testset = torchvision.datasets.CIFAR10(root='./dataCaps', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "#We load 4 samples per batchreduce the traininset to 2500\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=256, kernel_size=9):\n",
    "        \n",
    "        super(ConvLayer, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels,\n",
    "                               out_channels=out_channels,\n",
    "                               kernel_size=kernel_size,\n",
    "                               stride=1\n",
    "                             )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        #We want to transform 32x32x3 in 20x20x256 \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=32, in_channels=256, out_channels=8, kernel_size=9):\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "        self.capsules = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=num_capsules, kernel_size=kernel_size, stride=2, padding=0) \n",
    "                          for _ in range(out_channels)])\n",
    "    def forward(self, x):\n",
    "        u = [capsule(x) for capsule in self.capsules]\n",
    "        u = torch.stack(u, dim=4) \n",
    "        u = u.view(x.size(0), 32 * 8 * 8, -1)\n",
    "        #change shape of tensor to 32 capsules of 8x8x8  gridsize=8\n",
    "        return self.squash(u)\n",
    "    \n",
    "    #Squash function maintaining the direction of the vector (use instead of RELU activation fct in CNN)\n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm)+ 1e-8)\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=10, num_routes=32 * 8 * 8, in_channels=8, out_channels=16):\n",
    "        super(DigitCaps, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_routes = num_routes\n",
    "        self.num_capsules = num_capsules\n",
    "\n",
    "        \n",
    "        self.W = nn.Parameter(0.01*torch.randn(1, num_capsules, num_routes, out_channels, in_channels))#0.01 added from danielhavir\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.unsqueeze(4)\n",
    "        \n",
    "        \n",
    "        u_hat = torch.matmul(self.W, x) #matmul = matrix multiplication\n",
    "        \n",
    "        u_hat = u_hat.squeeze(-1)\n",
    "        temp_u_hat = u_hat.detach()\n",
    "        \n",
    "        b_ij = torch.zeros(batch_size,  self.num_capsules, self.num_routes,1)\n",
    "        num_iterations = 3 #number of routing\n",
    "\n",
    "        #this is the routing algorithm\n",
    "        for iteration in range(num_iterations-1):\n",
    "            #compute coupling coefficient, Conceptually: measure how likely capsule i may activate capsule j\n",
    "            c_ij = F.softmax(b_ij, dim=1)\n",
    "            \n",
    "            s_j = (c_ij * temp_u_hat).sum(dim=2)\n",
    "            \n",
    "            v_j = self.squash(s_j)\n",
    "            delta = torch.matmul(temp_u_hat, v_j.unsqueeze(-1))\n",
    "            b_ij = b_ij + delta\n",
    "        \n",
    "        c_ij = F.softmax(b_ij, dim=2)\n",
    "        s_j = (c_ij * u_hat).sum(dim=2)\n",
    "        \n",
    "        v_j = self.squash(s_j)\n",
    "        \n",
    "        return v_j\n",
    "    \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module): #difference between decoder and reconstruction ????\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "#num capsule=10, capsule_size=16, imsize=32, img_channel=3        \n",
    "        self.reconstraction_layers = nn.Sequential(\n",
    "            nn.Linear(16 * 10, 512), #nn.Linear(capsule_size*num_capsules, 512),\n",
    "            #nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            #nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 3072), #nn.Linear(1024, imsize*imsize*img_channel),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, output):\n",
    "        batch_size = output.size(0)\n",
    "        classes = torch.norm(output, dim=2)\n",
    "        \n",
    "        \n",
    "        _, max_length_indices = classes.max(dim=1)\n",
    "\n",
    "        masked = torch.eye(10)\n",
    "    \n",
    "        masked = masked.index_select(dim=0, index=max_length_indices).unsqueeze(2)\n",
    "        reconstructions = self.reconstraction_layers( (output*masked).view(batch_size, -1) )\n",
    "        reconstructions = reconstructions.view(-1, 3, 32, 32)\n",
    "        return reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.conv_layer = ConvLayer()\n",
    "        self.primary_capsules = PrimaryCaps()\n",
    "        self.digit_capsules = DigitCaps()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print ('original:',x.size()) #size=4,3,32,32\n",
    "        output = self.conv_layer(x)\n",
    "        #print(\"conv:\",output.size()) #size=4,256,24,24\n",
    "        output = self.primary_capsules(output)\n",
    "        #print(\"prim:\",output.size()) #size=4,2048,8\n",
    "        output = self.digit_capsules(output)\n",
    "        #print(\"digit:\",output.size()) #size=4,10,16\n",
    "        preds = torch.norm(output, dim=-1)\n",
    "        reconstructions = self.decoder(output)\n",
    "        return preds, reconstructions\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarginLoss(nn.Module):\n",
    "    def __init__(self, size_average=False, loss_lambda=0.5):\n",
    "        super(MarginLoss, self).__init__()\n",
    "        self.size_average = size_average\n",
    "        self.m_plus = 0.9\n",
    "        self.m_minus = 0.1\n",
    "        self.loss_lambda = loss_lambda\n",
    "\n",
    "    def forward(self, inputs, labels):\n",
    "        \n",
    "        left=F.relu(self.m_plus - inputs)**2\n",
    "        right=F.relu(inputs - self.m_minus)**2\n",
    "        L_k = labels * left + self.loss_lambda * (1 - labels) * right\n",
    "        L_k = L_k.sum(dim=1)\n",
    "        if self.size_average:\n",
    "            return L_k.mean()\n",
    "        else:\n",
    "            return L_k.sum()\n",
    "\n",
    "class CapsuleLoss(nn.Module):\n",
    "    def __init__(self, loss_lambda=0.5, recon_loss_scale=5e-4, size_average=False):\n",
    "\n",
    "        super(CapsuleLoss, self).__init__()\n",
    "        self.size_average = size_average\n",
    "        self.margin_loss = MarginLoss(size_average=size_average, loss_lambda=loss_lambda)\n",
    "        self.reconstruction_loss = nn.MSELoss(size_average=size_average)\n",
    "        self.recon_loss_scale = recon_loss_scale\n",
    "\n",
    "    def forward(self, inputs, labels, images, reconstructions):\n",
    "        margin_loss = self.margin_loss(inputs, labels)\n",
    "        reconstruction_loss = self.reconstruction_loss(reconstructions, images)\n",
    "        caps_loss = (margin_loss + self.recon_loss_scale * reconstruction_loss)\n",
    "\n",
    "        return caps_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1,   500] loss: 0.923, accuracy: 16.400\n",
      "[1,  1000] loss: 0.842, accuracy: 25.400\n",
      "[1,  1500] loss: 0.816, accuracy: 30.250\n",
      "[1,  2000] loss: 0.795, accuracy: 32.600\n",
      "[1,  2500] loss: 0.783, accuracy: 34.850\n",
      "[1,  3000] loss: 0.774, accuracy: 37.900\n",
      "[1,  3500] loss: 0.766, accuracy: 38.450\n",
      "[1,  4000] loss: 0.764, accuracy: 39.200\n",
      "[1,  4500] loss: 0.751, accuracy: 41.250\n",
      "[1,  5000] loss: 0.747, accuracy: 40.800\n",
      "[1,  5500] loss: 0.737, accuracy: 43.250\n",
      "[1,  6000] loss: 0.750, accuracy: 40.800\n",
      "[1,  6500] loss: 0.740, accuracy: 43.250\n",
      "[1,  7000] loss: 0.737, accuracy: 44.550\n",
      "[1,  7500] loss: 0.741, accuracy: 43.800\n",
      "[1,  8000] loss: 0.723, accuracy: 45.750\n",
      "[1,  8500] loss: 0.719, accuracy: 46.900\n",
      "[1,  9000] loss: 0.728, accuracy: 45.950\n",
      "[1,  9500] loss: 0.714, accuracy: 46.950\n",
      "[1, 10000] loss: 0.725, accuracy: 45.800\n",
      "[1, 10500] loss: 0.730, accuracy: 45.650\n",
      "[1, 11000] loss: 0.720, accuracy: 47.650\n",
      "[1, 11500] loss: 0.722, accuracy: 46.300\n",
      "[1, 12000] loss: 0.709, accuracy: 48.550\n",
      "[1, 12500] loss: 0.713, accuracy: 46.400\n",
      "[2,   500] loss: 0.700, accuracy: 50.150\n",
      "[2,  1000] loss: 0.705, accuracy: 48.650\n",
      "[2,  1500] loss: 0.702, accuracy: 48.850\n",
      "[2,  2000] loss: 0.712, accuracy: 48.450\n",
      "[2,  2500] loss: 0.703, accuracy: 48.150\n",
      "[2,  3000] loss: 0.697, accuracy: 48.250\n",
      "[2,  3500] loss: 0.690, accuracy: 50.250\n",
      "[2,  4000] loss: 0.689, accuracy: 50.500\n",
      "[2,  4500] loss: 0.693, accuracy: 51.300\n",
      "[2,  5000] loss: 0.692, accuracy: 49.350\n",
      "[2,  5500] loss: 0.686, accuracy: 52.200\n",
      "[2,  6000] loss: 0.683, accuracy: 51.800\n",
      "[2,  6500] loss: 0.685, accuracy: 53.550\n",
      "[2,  7000] loss: 0.682, accuracy: 52.900\n",
      "[2,  7500] loss: 0.662, accuracy: 52.300\n",
      "[2,  8000] loss: 0.673, accuracy: 52.050\n",
      "[2,  8500] loss: 0.676, accuracy: 52.450\n",
      "[2,  9000] loss: 0.667, accuracy: 53.900\n",
      "[2,  9500] loss: 0.682, accuracy: 52.450\n",
      "[2, 10000] loss: 0.666, accuracy: 55.150\n",
      "[2, 10500] loss: 0.663, accuracy: 55.000\n",
      "[2, 11000] loss: 0.663, accuracy: 54.100\n",
      "[2, 11500] loss: 0.656, accuracy: 56.750\n",
      "[2, 12000] loss: 0.660, accuracy: 55.350\n",
      "[2, 12500] loss: 0.661, accuracy: 56.550\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Training network over 2 epochs\n",
    "capsule_net = CapsNet()\n",
    "\n",
    "# Optimizer: Adam\n",
    "criterion = CapsuleLoss()\n",
    "optimizer = Adam(capsule_net.parameters(), lr=1e-3)\n",
    "#optimizer = optim.SGD(capsule_net.parameters(), lr=0.001, momentum=0.9)\n",
    "epochs=2\n",
    "capsule_net.train()\n",
    "total = 0\n",
    "correct = 0\n",
    "eye = torch.eye(len(classes))\n",
    "\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    writer = SummaryWriter(logdir='CapsNet_10/training_epoch%d'%(epoch))\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    eye = torch.eye(len(classes))\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, reconstructions = capsule_net(inputs)\n",
    "        labels_eye = eye[labels]\n",
    "        loss = criterion(outputs, labels_eye, inputs, reconstructions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        _,predicted=torch.max(outputs,1)\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        total += labels.size(0) #count the number of labels with right shape\n",
    "        correct += (predicted == labels).sum().item() #count the number of correct prediction\n",
    "        \n",
    "        if i % 500 == 499:    # print every 500 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f, accuracy: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000, 100*float(correct)/float(total)))\n",
    "            \n",
    "            writer.add_scalar('accuracy_train', 100*float(correct)/float(total), i)\n",
    "            writer.add_scalar('loss_train', running_loss / 2000, i)\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "    \n",
    "    #added check point\n",
    "    if epoch%2 == 2:\n",
    "        PATH = './checkpoint.1.pth'\n",
    "        torch.save(capsule_net.state_dict(), PATH)\n",
    "        \n",
    "    writer.close()\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training saving\n",
    "PATH = './cifar_CapsNet_SGD_2epochs_3.10.pth'\n",
    "torch.save(capsule_net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[  500] loss: 0.646, accuracy: 55.150\n",
      "[ 1000] loss: 0.642, accuracy: 56.300\n",
      "[ 1500] loss: 0.658, accuracy: 56.950\n",
      "[ 2000] loss: 0.651, accuracy: 55.300\n",
      "[ 2500] loss: 0.666, accuracy: 53.900\n",
      "Accuracy of the network on the 10000 test images: 55 %\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "PATH = './cifar_CapsNet_SGD_2epochs_3.10.pth'\n",
    "#Load previously net from choosen training\n",
    "capsule_net = CapsNet()\n",
    "capsule_net.load_state_dict(torch.load(PATH))\n",
    "criterion = CapsuleLoss()\n",
    "optimizer = Adam(capsule_net.parameters(), lr=1e-3)\n",
    "capsule_net.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "correct_final = 0\n",
    "total_final = 0\n",
    "eye = torch.eye(len(classes))\n",
    "\n",
    "writer = SummaryWriter(logdir='CapsNet_10/test.2')\n",
    "for i, datas in enumerate(testloader, 0):\n",
    "    inputs, labels = datas\n",
    "    \n",
    "    target = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "        \n",
    "    optimizer.zero_grad()    \n",
    "    output, reconstructions = capsule_net(inputs)\n",
    "    \n",
    "    _,predicted=torch.max(output,1)\n",
    "    labels_eye = eye[labels]\n",
    "    loss = criterion(output, labels_eye, inputs, reconstructions)\n",
    "    test_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    total += labels.size(0) #count the number of labels with right shape\n",
    "    correct += (predicted == labels).sum().item() #count the number of right labels\n",
    "    total_final += labels.size(0) #count the number of labels with right shape\n",
    "    correct_final += (predicted == labels).sum().item() #count the number of right labels \n",
    "    if i % 500 == 499:    # print every 500 mini-batches\n",
    "        print('[%5d] loss: %.3f, accuracy: %.3f' %( i + 1, (test_loss / 2000), (100*correct/total) ))\n",
    "        \n",
    "        writer.add_scalar('accuracy_test', 100*correct/total, i)\n",
    "        writer.add_scalar('loss_test', test_loss / 2000, i)\n",
    "        \n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0    \n",
    "writer.close()\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct_final / total_final))    \n",
    "    \n",
    "print (test_loss / len(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0679cfc34d88478dc6da67029260a6ae088549fe514c33848ba6648c7fb36b966",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}