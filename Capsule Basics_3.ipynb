{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "answering-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from matplotlib.pyplot import *\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "residential-procedure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./dataCaps/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90fe5b177044054bbe1a7f6b20c8975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataCaps/cifar-10-python.tar.gz to ./dataCaps\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Load train set and test set and normalize the images in range [-1,1]\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#50000 images training\n",
    "trainset = torchvision.datasets.CIFAR10(root='./dataCaps', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "#We load 4 samples per batchreduce the traininset to 12500\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)#batch size changed from 4 to 100\n",
    "\n",
    "#print(len(trainset))\n",
    "#10000 images test\n",
    "testset = torchvision.datasets.CIFAR10(root='./dataCaps', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)#batch size changed from 4 to 100\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "operating-skill",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=256, kernel_size=9):\n",
    "        \n",
    "        super(ConvLayer, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels,\n",
    "                               out_channels=out_channels,\n",
    "                               kernel_size=kernel_size,\n",
    "                               stride=1\n",
    "                             )\n",
    "        #We want to transform 32x32x3 in 20x20x256 \n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "collect-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=32, in_channels=256, out_channels=8, kernel_size=9):\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "        self.capsules = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=num_capsules, kernel_size=kernel_size, stride=2, padding=0) \n",
    "                          for _ in range(out_channels)])\n",
    "    def forward(self, x):\n",
    "        u = [capsule(x) for capsule in self.capsules]\n",
    "        u = torch.stack(u, dim=4) #it was 1 here but it is like this\n",
    "        u = u.view(x.size(0), 32 * 8 * 8, -1)\n",
    "        #output = output.view(x.size(0), self.num_capsules*(self.gridsize)*(self.gridsize), -1)\n",
    "        #change shape of tensor to 32 capsules of 8x8x8  gridsize=8\n",
    "        return self.squash(u)\n",
    "    \n",
    "    #Squash function maintaining the direction of the vector (use instead of RELU activation fct in CNN)\n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "applicable-couple",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=10, num_routes=32 * 8 * 8, in_channels=8, out_channels=16):\n",
    "        super(DigitCaps, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_routes = num_routes\n",
    "        self.num_capsules = num_capsules\n",
    "\n",
    "        \n",
    "        self.W = nn.Parameter(torch.randn(1, num_routes, num_capsules, out_channels, in_channels))\n",
    "        self.bias = nn.Parameter(torch.rand(4,1, num_capsules, out_channels))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        \n",
    "        x = torch.stack([x] * self.num_capsules, dim=2).unsqueeze(4)\n",
    "        #x = x.unsqueeze(2).unsqueeze(4)\n",
    "        W = torch.cat([self.W] * batch_size, dim=0) #concatenate in zero dimension\n",
    "        \n",
    "        u_hat = torch.matmul(W, x) #matmul = matrix multiplication\n",
    "        u_hat = u_hat.squeeze()\n",
    "        num_capsules_in = x.shape[1]\n",
    "        num_capsules_out = W.shape[2]\n",
    "        \n",
    "        #b_ij = Variable(x.new(batch_size, num_capsules_in, num_capsules_out, 1).zero_())\n",
    "        b_ij = Variable(torch.zeros(1, self.num_routes, self.num_capsules, 1))\n",
    "        \n",
    "        \n",
    "        num_iterations = 3 #number of routing\n",
    "        #this is the routing algorithm\n",
    "        for iteration in range(num_iterations):\n",
    "            #compute coupling coefficient, Conceptually: measure how likely capsule i may activate capsule j\n",
    "            c_ij = F.softmax(b_ij, dim=2)\n",
    "            \n",
    "            c_ij = torch.cat([c_ij], dim=0)\n",
    "            #print(\"c_ij\",c_ij.size(),\"uhat\",u_hat.size()),\"bij\"\n",
    "            \n",
    "            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n",
    "            #print(\"ok\")\n",
    "            #s_j = s_j + self.bias #before without bias\n",
    "            \n",
    "            v_j = self.squash(s_j)\n",
    "            delta = (u_hat * v_j).sum(dim=0, keepdim=True)\n",
    "            b_ij = b_ij + delta\n",
    "        return v_j.squeeze(1)\n",
    "    \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "olive-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module): #difference between decoder and reconstruction ????\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "#num capsule=10, capsule_size=16, imsize=32, img_channel=3        \n",
    "        self.reconstraction_layers = nn.Sequential(\n",
    "            nn.Linear(16 * 10, 512), #nn.Linear(capsule_size*num_capsules, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 3072), #nn.Linear(1024, imsize*imsize*img_channel),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, output, data):\n",
    "        batch_size = output.size(0)\n",
    "        classes = torch.norm(output, dim=2)\n",
    "        classes = F.softmax(classes)\n",
    "        max_length_indices = classes.max(dim=1)[1].squeeze()\n",
    "        #_, max_length_indices = classes.max(dim=1)\n",
    "        #masked = Variable(torch.sparse.torch.eye(10)) #Variable(torch.sparse.torch.eye(num_capsules))\n",
    "        #masked = masked.index_select(dim=0, index=max_length_indices.squeeze(1).data)\n",
    "        \n",
    "        #reconstructions = self.reconstraction_layers((x * masked[:, :, None, None]).view(x.size(0), -1))\n",
    "        #reconstructions = reconstructions.view(-1, 3, 32, 32) #reconstructions.view(-1, 1, 28, 28)\n",
    "        masked = Variable(output.new_tensor(torch.eye(10)))\n",
    "    \n",
    "        masked = masked.index_select(dim=0, index=max_length_indices.data)\n",
    "        #print(\"x\",x.size(),\"masked\",masked[:, :, None, None].size())\n",
    "        decoder_input = (output * masked[:, :, None, None].squeeze(3)).view(batch_size, -1)\n",
    "\n",
    "        reconstructions = self.reconstraction_layers(decoder_input)\n",
    "        reconstructions = reconstructions.view(-1, 3, 32, 32)\n",
    "        return reconstructions, masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "atlantic-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.conv_layer = ConvLayer()\n",
    "        self.primary_capsules = PrimaryCaps()\n",
    "        self.digit_capsules = DigitCaps()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, x, target=None):\n",
    "        output = self.conv_layer(x)\n",
    "        #print(\"conv:\",output.size())\n",
    "        output = self.primary_capsules(output)\n",
    "        #print(\"prim:\",output.size())\n",
    "        output = self.digit_capsules(output)\n",
    "        #print(\"digit:\",output.size())\n",
    "        reconstructions, masked = self.decoder(output, target)\n",
    "        return output, reconstructions, masked\n",
    "    \n",
    "    def loss(self, dataset_im, dataset_labs, caps_output, reconstructions):\n",
    "        #return self.margin_loss(x, target) + self.reconstruction_loss(data, reconstructions)\n",
    "        marg_loss = self.margin_loss(caps_output, dataset_labs)\n",
    "        rec_loss = self.reconstruction_loss(dataset_im, reconstructions)\n",
    "        total_loss = (marg_loss + 0.0005 * rec_loss).mean() #0.0005=regularization factor\n",
    "        return total_loss, rec_loss.mean(), marg_loss.mean()\n",
    "    \n",
    "    def margin_loss(self, y_pred, labels):\n",
    "        #lambda = 0.5 \n",
    "        #print (\"test\", torch.max(0, 0.9 - torch.norm(y_pred)))\n",
    "        \n",
    "        norm_y_pred = torch.norm(y_pred.type(torch.FloatTensor))\n",
    "        left = labels * torch.max(torch.tensor([0, 0.9 - norm_y_pred]))**2\n",
    "        x = torch.max(torch.tensor([0, norm_y_pred - 0.1]))**2\n",
    "        right = 0.5 * (1 - labels) * x\n",
    "        loss = left + right\n",
    "        return loss\n",
    "    \n",
    "    def reconstruction_loss(self, data, reconstructions):\n",
    "        batch_size = reconstructions.size(0)\n",
    "        \n",
    "        reconstructions = reconstructions.view(batch_size, -1)\n",
    "        #print('data',data[0])\n",
    "        data = data.view(batch_size, -1)#here it was data[0]\n",
    "        #print('rec',(reconstructions).size(),'data',data.size())\n",
    "        loss = nn.MSELoss()(reconstructions,data)\n",
    "        #self.mse_loss(reconstructions.view(reconstructions.size(0), -1), data.view(reconstructions.size(0), -1))\n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ongoing-negative",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-7d9a11db3d4a>:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  classes = F.softmax(classes)\n",
      "<ipython-input-9-7d9a11db3d4a>:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  masked = Variable(output.new_tensor(torch.eye(10)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: -150.004\n",
      "[1,  4000] loss: -150.035\n",
      "[1,  6000] loss: -147.596\n",
      "[1,  8000] loss: -153.665\n",
      "[1, 10000] loss: -145.743\n",
      "[1, 12000] loss: -146.295\n",
      "[2,  2000] loss: -143.982\n",
      "[2,  4000] loss: -145.568\n",
      "[2,  6000] loss: -146.401\n",
      "[2,  8000] loss: -146.178\n",
      "[2, 10000] loss: -154.472\n",
      "[2, 12000] loss: -144.134\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Training network over 2 epochs\n",
    "capsule_net = CapsNet()\n",
    "#Define Loss function and optimizer\n",
    "# Loss Function: cross entropy\n",
    "# Optimizer: SGD\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = Adam(capsule_net.parameters())\n",
    "optimizer = optim.SGD(capsule_net.parameters(), lr=0.001, momentum=0.9)\n",
    "epochs=2\n",
    "capsule_net.train()\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "#print('inputs',inputs.size(),'labels',labels.size())\n",
    "total = 0\n",
    "correct = 0\n",
    "writer = SummaryWriter(logdir='CapsNet_3/accuracy')\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, reconstructions, masked = capsule_net(inputs)\n",
    "        _, predicted =torch.max(masked, 1)\n",
    "        #print(\"label_gdtrue:\",labels,\"predicted\",predicted)\n",
    "        #print('inputs',inputs.size(),'labels',labels,'outpu',outputs.size(), 'rec', reconstructions.size())\n",
    "        #print(outputs)\n",
    "        \n",
    "        loss, rec_loss, marginal_loss = capsule_net.loss(inputs, labels, predicted, reconstructions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print('epoch:',epoch,'batch:',i)\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        total += labels.size(0) #count the number of labels with right shape\n",
    "        correct += (predicted == labels).sum().item() #count the number \n",
    "        \n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "            writer.add_scalar('accuracy', correct/total, i)\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "writer.close()\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unable-decline",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training saving\n",
    "PATH = './cifar_CapsNet_SGD_2epochs_3.1.pth'\n",
    "torch.save(capsule_net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "artistic-pavilion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-7d9a11db3d4a>:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  classes = F.softmax(classes)\n",
      "<ipython-input-9-7d9a11db3d4a>:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  masked = Variable(output.new_tensor(torch.eye(10)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 10 %\n",
      "-62.770535162907976\n"
     ]
    }
   ],
   "source": [
    "PATH = './cifar_CapsNet_SGD_2epochs_3.pth'\n",
    "#Load previously net from choosen training\n",
    "capsule_net = CapsNet()\n",
    "capsule_net.load_state_dict(torch.load(PATH))\n",
    "\n",
    "\n",
    "capsule_net.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for i, datas in enumerate(testloader, 0):\n",
    "    inputs, labels = datas\n",
    "    \n",
    "    target = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "        \n",
    "        \n",
    "    output, reconstructions, masked = capsule_net(inputs)\n",
    "    \n",
    "    loss, rec_loss, marg_loss = capsule_net.loss(inputs, labels, output, reconstructions)\n",
    "    test_loss += loss.item()\n",
    "    \n",
    "    _, predicted =torch.max(masked, 1)\n",
    "    total += labels.size(0) #count the number of labels with right shape\n",
    "    correct += (predicted == labels).sum().item() #count the number of right labels \n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))    \n",
    "    \n",
    "print (test_loss / len(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-genesis",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
